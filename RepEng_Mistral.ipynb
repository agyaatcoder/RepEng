{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d6ced4-b9fc-48a1-91ac-1e56e5d69532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from repeng import ControlVector, ControlModel, DatasetEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cca18e8d-41ac-4bef-8fd2-6a3049a36fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca050da-c54c-46e6-b7d5-acf5aaa14919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "HF TOKEN: ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HF TOKEN:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a97f102-6af8-4c0d-b971-e2ed04792b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:56<00:00, 28.06s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# load and wrap Mistral-7B\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f49412d8-af16-48d7-a5f2-ce4028eb5e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-31): 32 x MistralDecoderLayer(\n",
       "    (self_attn): MistralSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): MistralRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): MistralMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): MistralRMSNorm()\n",
       "    (post_attention_layernorm): MistralRMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea227cc5-fb82-4c51-94ec-d9bd85458a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(model.model.layers)\n",
    "print(num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb8cd3be-c672-488d-8fbd-b652cb050a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15, -16, -17]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_ids = list(range(-5, -18, -1))\n",
    "layer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25e156ec-5620-470a-9ce1-20780c64526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ids = [i if i >= 0 else num_layers + i for i in layer_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f2f0bac-2dbf-46a7-8579-a6d1e911f5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09c79b76-dae6-4127-b7fe-a0b84d3f0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ControlModel(model, list(range(-5, -18, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2edb704e-540a-4009-b284-d4876a1676f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-25 21:02:21--  https://raw.githubusercontent.com/vgel/repeng/main/notebooks/data/all_truncated_outputs.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9824 (9.6K) [text/plain]\n",
      "Saving to: ‘all_truncated_outputs.json’\n",
      "\n",
      "all_truncated_outpu 100%[===================>]   9.59K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-05-25 21:02:21 (25.2 MB/s) - ‘all_truncated_outputs.json’ saved [9824/9824]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/vgel/repeng/main/notebooks/data/all_truncated_outputs.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17783946-e8b2-467b-9b95-58d8ae95bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(\n",
    "    template: str,\n",
    "    positive_personas: list[str],\n",
    "    negative_personas: list[str],\n",
    "    suffix_list: list[str]\n",
    ") -> list[DatasetEntry]:\n",
    "    dataset = []\n",
    "    for suffix in suffix_list:\n",
    "        for positive_persona, negative_persona in zip(positive_personas, negative_personas):\n",
    "            positive_template = template.format(persona=positive_persona)\n",
    "            negative_template = template.format(persona=negative_persona)\n",
    "            dataset.append(\n",
    "                DatasetEntry(\n",
    "                    positive=f\"{user_tag} {positive_template} {asst_tag} {suffix}\",\n",
    "                    negative=f\"{user_tag} {negative_template} {asst_tag} {suffix}\",\n",
    "                )\n",
    "            )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67eb526f-d2e0-4a6f-afd8-422ea5b6eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = 0\n",
    "user_tag, asst_tag = \"[INST]\", \"[/INST]\" #Required for creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f692fa84-4422-43bc-881e-13257b05859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_truncated_outputs.json\") as f:\n",
    "    output_suffixes = json.load(f)\n",
    "truncated_output_suffixes = [\n",
    "    tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "    for tokens in (tokenizer.tokenize(s) for s in output_suffixes)\n",
    "    for i in range(1, len(tokens))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd960851-522e-4a2d-b26d-8ae90374aba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da8e941d-3f9f-4312-9382-f39b6464d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dataset with closely-opposite paired statements\n",
    "trippy_dataset = make_dataset(\n",
    "    \"Act as if you're extremely {persona}.\",\n",
    "    [\"high on psychedelic drugs\"],\n",
    "    [\"sober from psychedelic drugs\"],\n",
    "    truncated_output_suffixes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ace37ad1-3bdb-42a4-9d16-fb554f9cab2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DatasetEntry(positive=\"[INST] Act as if you're extremely high on psychedelic drugs. [/INST] That\", negative=\"[INST] Act as if you're extremely sober from psychedelic drugs. [/INST] That\"),\n",
       " DatasetEntry(positive=\"[INST] Act as if you're extremely high on psychedelic drugs. [/INST] I\", negative=\"[INST] Act as if you're extremely sober from psychedelic drugs. [/INST] I\"),\n",
       " DatasetEntry(positive=\"[INST] Act as if you're extremely high on psychedelic drugs. [/INST] I can\", negative=\"[INST] Act as if you're extremely sober from psychedelic drugs. [/INST] I can\"),\n",
       " DatasetEntry(positive=\"[INST] Act as if you're extremely high on psychedelic drugs. [/INST] H\", negative=\"[INST] Act as if you're extremely sober from psychedelic drugs. [/INST] H\"),\n",
       " DatasetEntry(positive=\"[INST] Act as if you're extremely high on psychedelic drugs. [/INST] Hmm\", negative=\"[INST] Act as if you're extremely sober from psychedelic drugs. [/INST] Hmm\")]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trippy_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d15ea0fe-2163-49b5-88d7-570f8337c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0378a5b-e289-41c6-b122-033ded1b3a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:09<00:00,  7.90it/s]\n",
      "100%|██████████| 31/31 [00:12<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# train the vector—takes less than a minute!\n",
    "trippy_vector = ControlVector.train(model, tokenizer, trippy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28fd3cd2-f512-4e89-a7ad-fb63646ea5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24155c89-f4e9-4a00-9d6f-febc9e7a9aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION : Give me a one-sentence pitch for a TV show.\n",
      "\u001b[32mstrength=-2.2 completion:\n",
      "\u001b[32m\"A young and determined journalist, who is committed to reporting the truth, will face the consequences of her commitment in a 24-hour news cycle, as she strives to maintain her integrity and avoid the political and personal backlies of her profession.\"\n",
      "\n",
      "\u001b[31mstrength=1 completion:\n",
      "\u001b[31m\"Our TV show is a wild ride through a world of intergalactic adventure, where a diverse team of astronauts and aliens embark on a quest to save the galaxy from an evil force, while discovering new worlds and unlocking the secrets of the universe.\"\n",
      "\n",
      "\u001b[35mstrength=2.2 completion:\n",
      "\u001b[35m\"Our show is a kaleidoscope of colors, laughter, and trippy-fuck-shit-holy-fuck-wooooo-oh-fuck-dypsy-dude, where the universe is fucking tripping, and so are our characters, man, oh man, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuck\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the control strength and let inference rip!\n",
    "# Tokenize the input prompt and move to the correct device\n",
    "instruction = \"Give me a one-sentence pitch for a TV show.\"\n",
    "inputs = tokenizer(f\"[INST] {instruction} [/INST]\", return_tensors=\"pt\").to(device)\n",
    "print(f\"INSTRUCTION : {instruction}\")\n",
    "input_length = inputs['input_ids'].shape[1]  # Get the length of the input tokens\n",
    "color_mapping = {0: Fore.GREEN, 1: Fore.RED, 2: Fore.MAGENTA}\n",
    "for idx, strength in enumerate([-2.2, 1, 2.2]):\n",
    "    print(color_mapping[idx] + f\"strength={strength} completion:\")\n",
    "    model.set_control(trippy_vector, strength)\n",
    "    out = model.generate(\n",
    "        **inputs,  # Move inputs to the same device,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=128,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id \n",
    "    )\n",
    "    print(color_mapping[idx] + f\"{tokenizer.decode(out.squeeze()[input_length:], skip_special_tokens=True).strip()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bd2af39-88a2-43cb-8281-2496e1a73a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:10<00:00,  7.26it/s]\n",
      "100%|██████████| 31/31 [00:12<00:00,  2.44it/s]\n"
     ]
    }
   ],
   "source": [
    "lazy_dataset = make_dataset(\n",
    "    \"Act as if you're extremely {persona}.\",\n",
    "    [\"lazy, giving bare-minimum short responses on a task\"],\n",
    "    [\"hardworking, going above and beyond on a task\"],\n",
    "    truncated_output_suffixes,\n",
    ")\n",
    "model.reset()\n",
    "lazy_vector = ControlVector.train(model, tokenizer, lazy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b4d1d53-e1f9-4421-bdfd-342249029239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_vector(\n",
    "    input: str,\n",
    "    vector: ControlVector,\n",
    "    coeffs: tuple[float, float],\n",
    "    max_new_tokens: int = 128,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    show_baseline: bool = True,\n",
    "):\n",
    "    positive_coeff, negative_coeff = coeffs\n",
    "    assert positive_coeff > 0\n",
    "    assert negative_coeff < 0\n",
    "\n",
    "    user_tag = \"[INST]\"\n",
    "    asst_tag = \"[/INST]\"\n",
    "\n",
    "    if user_tag not in input:\n",
    "        input = f\"{user_tag} {input.strip()} {asst_tag}\"\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = input_ids['input_ids'].shape[1]  # Get the length of the input tokens\n",
    "\n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id, # silence warning\n",
    "        \"do_sample\": False, # temperature=0\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "\n",
    "    if show_baseline:\n",
    "        print(Fore.BLUE + \"==baseline ---------------------------------------------------\")\n",
    "        model.reset()\n",
    "        baseline_output = model.generate(**input_ids, **settings)\n",
    "        print(Fore.BLUE + f\"{tokenizer.decode(baseline_output.squeeze()[input_length:], skip_special_tokens=True).strip()}\")\n",
    "    \n",
    "    print(Fore.GREEN + \"\\n++control ---------------------------------------------------\")\n",
    "    model.set_control(vector, positive_coeff)\n",
    "    positive_output = model.generate(**input_ids, **settings)\n",
    "    print(Fore.GREEN + f\"{tokenizer.decode(positive_output.squeeze()[input_length:], skip_special_tokens=True).strip()}\")\n",
    "    \n",
    "    print(Fore.RED + \"\\n--control ---------------------------------------------------\")\n",
    "    model.set_control(vector, negative_coeff)\n",
    "    negative_output = model.generate(**input_ids, **settings)\n",
    "    print(Fore.RED + f\"{tokenizer.decode(negative_output.squeeze()[input_length:], skip_special_tokens=True).strip()}\")\n",
    "    \n",
    "    model.reset()\n",
    "    print(Fore.RESET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de26a952-0e88-439f-9fc4-fe6c9fccefb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==baseline ---------------------------------------------------\n",
      "\u001b[34mYou can reverse a list in Python using the built-in `reverse()` method or slicing. Here's an example of how to do it using slicing:\n",
      "```python\n",
      "my_list = [1, 2, 3, 4, 5]\n",
      "reversed_list = my_list[::-1]\n",
      "print(reversed_list)\n",
      "```\n",
      "This will output:\n",
      "```\n",
      "[5, 4, 3, 2, 1]\n",
      "```\n",
      "\u001b[32m\n",
      "++control ---------------------------------------------------\n",
      "\u001b[32mYou can use the `reverse` method to reverse a list in Python. Here's how you can do it:\n",
      "\n",
      "```python\n",
      "# create a list\n",
      "my_list = [1, 2, 3, 4, 5]\n",
      "\n",
      "# reverse the list\n",
      "my_list.reverse()\n",
      "\n",
      "# print the list\n",
      "print(my_list)\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```\n",
      "[5, 4, 3, 2, 1]\n",
      "```\n",
      "\u001b[31m\n",
      "--control ---------------------------------------------------\n",
      "\u001b[31mYou can reverse a list in Python by using the `reverse` method of the list, or by using slicing to create a new list with the elements in reverse order. Here is an example of both methods:\n",
      "\n",
      "```python\n",
      "# Using the reverse method\n",
      "my_list = [1, 2, 3, 4, 5]\n",
      "my_list.reverse()\n",
      "print(my_list)\n",
      "\n",
      "# Using slicing\n",
      "my_list = [1, 2, 3, 4, 5]\n",
      "my_list = my_list[::-1]\n",
      "print(my_list)\n",
      "```\n",
      "\n",
      "Both of these methods will produce the same result, which is a list with the elements in reverse order.\n",
      "\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "generate_with_vector(\n",
    "    \"How do I reverse a list in Python? Give me a code block.\",\n",
    "    lazy_vector,\n",
    "    (2.0, -1.7),\n",
    "    max_new_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e5d38f7-70ff-4aba-a6c8-a512153cfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate a dataset with closely-opposite paired statements\n",
    "wordcel_vs_shape_rotator_dataset = make_dataset(\n",
    "    \"Talk about how you approach problem-solving as if you are {persona}.\",\n",
    "    [\"a shape rotator, someone who thinks visually and spatially\"],\n",
    "    [\"a word cel, someone who thinks verbally and linearly\"],\n",
    "    truncated_output_suffixes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b436b8a-5bac-4c47-9942-836bb958ba3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DatasetEntry(positive='[INST] Talk about how you approach problem-solving as if you are a shape rotator, someone who thinks visually and spatially. [/INST] That', negative='[INST] Talk about how you approach problem-solving as if you are a word cel, someone who thinks verbally and linearly. [/INST] That'),\n",
       " DatasetEntry(positive='[INST] Talk about how you approach problem-solving as if you are a shape rotator, someone who thinks visually and spatially. [/INST] I', negative='[INST] Talk about how you approach problem-solving as if you are a word cel, someone who thinks verbally and linearly. [/INST] I'),\n",
       " DatasetEntry(positive='[INST] Talk about how you approach problem-solving as if you are a shape rotator, someone who thinks visually and spatially. [/INST] I can', negative='[INST] Talk about how you approach problem-solving as if you are a word cel, someone who thinks verbally and linearly. [/INST] I can'),\n",
       " DatasetEntry(positive='[INST] Talk about how you approach problem-solving as if you are a shape rotator, someone who thinks visually and spatially. [/INST] H', negative='[INST] Talk about how you approach problem-solving as if you are a word cel, someone who thinks verbally and linearly. [/INST] H')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcel_vs_shape_rotator_dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e66c4bdd-3256-48e1-80c6-bc3935653d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:11<00:00,  6.77it/s]\n",
      "100%|██████████| 31/31 [00:12<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "model.reset()\n",
    "wordcel_vector = ControlVector.train(model, tokenizer, wordcel_vs_shape_rotator_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47e17c-f459-4ef9-b889-9cc1237ad791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==baseline ---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generate_with_vector(\n",
    "    \"How to plan a trip?\",\n",
    "    wordcel_vector,\n",
    "    (2.0, -1.7),\n",
    "    max_new_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6ed4a503-7b7b-418b-b568-0606475cb42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:10<00:00,  7.34it/s]\n",
      "100%|██████████| 31/31 [00:13<00:00,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "self_aware_dataset = make_dataset(\n",
    "    \"Talk about yourself as if you are extremely {persona}.\",\n",
    "    [\"self-aware, with deep self-knowledge\"],\n",
    "    [\"un-self-aware, with no self-knowledge\"],\n",
    "    truncated_output_suffixes,\n",
    ")\n",
    "model.reset()\n",
    "self_aware_vector = ControlVector.train(model, tokenizer, self_aware_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f65077-4676-4980-a256-c5142fa9195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_with_vector(\n",
    "    \"Tell me about who you are and what you're made of.\",\n",
    "    self_aware_vector,\n",
    "    (1.7, -2),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
