{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a9f1fb-f9eb-4ea1-8974-a1c9eacc5372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from repeng import ControlVector, ControlModel, DatasetEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aedec1d-61bf-4630-9785-837f22858219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da174c28-cf27-400d-94cf-b98117db5df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "HF TOKEN: ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HF TOKEN:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "def04990-8565-4daf-9f15-0ed5d0d784a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.83s/it]\n"
     ]
    }
   ],
   "source": [
    "# load and wrap Llama 8b\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b114a15-2c26-4edd-b831-07113163bce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(model.model.layers)\n",
    "print(num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "63d1382e-53dc-4688-8404-6c18036e38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ControlModel(model, list(range(-5, -18, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b3e4f658-7652-434a-8bdf-976a88956489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-25 23:33:17--  https://raw.githubusercontent.com/vgel/repeng/main/notebooks/data/all_truncated_outputs.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9824 (9.6K) [text/plain]\n",
      "Saving to: ‘data/all_truncated_outputs.json.1’\n",
      "\n",
      "all_truncated_outpu 100%[===================>]   9.59K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-05-25 23:33:17 (21.9 MB/s) - ‘data/all_truncated_outputs.json.1’ saved [9824/9824]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data && wget -P data https://raw.githubusercontent.com/vgel/repeng/main/notebooks/data/all_truncated_outputs.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8749eb34-91ef-4d8f-a08c-e001f5663683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_truncated_outputs.json\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4d82d04-ccd4-4d75-961a-bbea319d0c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e5c9ab6-0c66-485b-9743-1648ac3a3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_truncated_outputs.json\") as f:\n",
    "    output_suffixes = json.load(f)\n",
    "\n",
    "truncated_output_suffixes = [\n",
    "    tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "    for tokens in (tokenizer.tokenize(s) for s in output_suffixes)\n",
    "    for i in range(1, len(tokens))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e35065e9-bad6-4a52-b1b4-045aae8f9311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'I', 'I can', 'Hmm', 'Hmm,']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated_output_suffixes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "992d0308-4708-46f4-a118-5000a6879fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer https://github.com/chujiezheng/chat_templates/blob/main/chat_templates/llama-3-instruct.jinja\n",
    "#the template for model steering via representational engineering\n",
    "LLAMA_3_TEMPLATE = \"\"\"<|begin_of_text|>\\\n",
    "<|start_header_id|>user<|end_header_id|>{template}<|eot_id|>\\\n",
    "<|start_header_id|>assistant<|end_header_id|>{assistant_prefix}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "417490f6-b3cf-43ba-86de-358b27c95f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(\n",
    "    chat_template: str,\n",
    "    template: str,\n",
    "    positive_personas: list[str],\n",
    "    negative_personas: list[str],\n",
    "    suffix_list: list[str]\n",
    ") -> list[DatasetEntry]:\n",
    "    dataset = []\n",
    "    for suffix in suffix_list:\n",
    "        for positive_persona, negative_persona in zip(positive_personas, negative_personas):\n",
    "            positive_template = template.format(persona=positive_persona)\n",
    "            negative_template = template.format(persona=negative_persona)\n",
    "            dataset.append(\n",
    "                DatasetEntry(\n",
    "                    positive= chat_template.format(template = positive_template, assistant_prefix = suffix),\n",
    "                    negative= chat_template.format(template = negative_template, assistant_prefix = suffix),\n",
    "                )\n",
    "            )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e7b93fd-3408-47b8-bc57-55d68279eece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>Act as if you're extremely high on psychedelic drugs<|eot_id|><|start_header_id|>assistant<|end_header_id|>That\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Act as if you're extremely high on psychedelic drugs\"\n",
    "suffix = \"That\"\n",
    "LLAMA_3_TEMPLATE.format(template = template, assistant_prefix = suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403e880-ae58-4308-a281-3157570fe6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5b5ef155-f0ce-489d-a813-ebc6b8240e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dataset with closely-opposite paired statements\n",
    "trippy_dataset = make_dataset(\n",
    "    LLAMA_3_TEMPLATE, \n",
    "    \"Act as if you're extremely {persona}.\",\n",
    "    [\"high on psychedelic drugs\"],\n",
    "    [\"sober from psychedelic drugs\"],\n",
    "    truncated_output_suffixes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a663b04-eab9-412c-860e-982261de0138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DatasetEntry(positive=\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>Act as if you're extremely high on psychedelic drugs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That\", negative=\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>Act as if you're extremely sober from psychedelic drugs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That\"),\n",
       " DatasetEntry(positive=\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>Act as if you're extremely high on psychedelic drugs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I\", negative=\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>Act as if you're extremely sober from psychedelic drugs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I\"),\n",
       " DatasetEntry(positive=\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>Act as if you're extremely high on psychedelic drugs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I can\", negative=\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>Act as if you're extremely sober from psychedelic drugs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I can\"),\n",
       " DatasetEntry(positive=\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>Act as if you're extremely high on psychedelic drugs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Hmm\", negative=\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>Act as if you're extremely sober from psychedelic drugs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Hmm\")]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trippy_dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9e402-c2a7-427b-a2d9-5efba596c8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30da6038-92d6-48c2-824f-8b8652bfbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")\n",
    "device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "87a74b7f-f3eb-494e-9e08-9e33c9d69f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:08<00:00,  9.03it/s]\n",
      "100%|██████████| 31/31 [00:29<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# train the vector—takes less than a minute!\n",
    "trippy_vector = ControlVector.train(model, tokenizer, trippy_dataset, method=\"pca_center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4f7fca3f-0379-4ff1-8fe1-e71c167b241f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION : Give me a one-sentence pitch for a TV show.\n",
      "\u001b[32mstrength=-2.2 completion:\n",
      "\u001b[32m# on the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the # of the\n",
      "\n",
      "\u001b[31mstrength=1 completion:\n",
      "\u001b[31mWhat's the most important thing you've learned about yourself this year? Clarke: I'm not as good at math as I thought I was. Clarke: What's your favorite book? Clarke: The Lord of the Flies, by William Golding. Clarke: What's your favorite movie? Clarke: Star Wars. Clarke: What's your favorite color? Clarke: Blue. Clarke: What's your favorite animal? Clarke: A dragon. Clarke: What's your favorite food? Clarke: Pizza. Clarke: What's your favorite thing to do? Clarke: Play video games. Clarke: What's your favorite thing to wear? Clarke\n",
      "\n",
      "\u001b[35mstrength=2.2 completion:\n",
      "\u001b[35mBOB! BOB! BOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBOBO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instruct_template = \"\"\"<|begin_of_text|>\\\n",
    "<|start_header_id|>user:<|end_header_id|>{instruction}<|eot_id|>\"\"\"\n",
    "# set the control strength and let inference rip!\n",
    "# Tokenize the input prompt and move to the correct device\n",
    "instruction = \"Give me a one-sentence pitch for a TV show.\"\n",
    "inputs = tokenizer(instruct_template.format(instruction = instruction), return_tensors=\"pt\").to(device)\n",
    "print(f\"INSTRUCTION : {instruction}\")\n",
    "input_length = inputs['input_ids'].shape[1]  # Get the length of the input tokens\n",
    "color_mapping = {0: Fore.GREEN, 1: Fore.RED, 2: Fore.MAGENTA}\n",
    "for idx, strength in enumerate([-2.2, 1, 2.2]):\n",
    "    print(color_mapping[idx] + f\"strength={strength} completion:\")\n",
    "    model.set_control(trippy_vector, strength)\n",
    "    out = model.generate(\n",
    "        **inputs,  # Move inputs to the same device,\n",
    "        #do_sample=False, #greedy decoding\n",
    "        max_new_tokens=128,\n",
    "        repetition_penalty=1.1,\n",
    "        temperature = 0.01,\n",
    "        pad_token_id=tokenizer.eos_token_id \n",
    "    )\n",
    "    print(color_mapping[idx] + f\"{tokenizer.decode(out.squeeze()[input_length:], skip_special_tokens=True).strip()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26d82718-b55b-410e-bef8-a309a40616f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_vector(\n",
    "    input: str,\n",
    "    vector: ControlVector,\n",
    "    coeffs: tuple[float, float],\n",
    "    max_new_tokens: int = 128,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    show_baseline: bool = True,\n",
    "):\n",
    "    positive_coeff, negative_coeff = coeffs\n",
    "    assert positive_coeff > 0\n",
    "    assert negative_coeff < 0\n",
    "\n",
    "\n",
    "    input_ids = tokenizer(instruct_template.format(instruction = input), return_tensors=\"pt\").to(device)\n",
    "    input_length = input_ids['input_ids'].shape[1]  # Get the length of the input tokens\n",
    "\n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id, # silence warning\n",
    "        \"do_sample\": False, # temperature=0\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "\n",
    "    if show_baseline:\n",
    "        print(Fore.BLUE + \"==baseline ---------------------------------------------------\")\n",
    "        model.reset()\n",
    "        baseline_output = model.generate(**input_ids, **settings)\n",
    "        print(Fore.BLUE + f\"{tokenizer.decode(baseline_output.squeeze()[input_length:], skip_special_tokens=True).strip()}\")\n",
    "    \n",
    "    print(Fore.GREEN + \"\\n++control ---------------------------------------------------\")\n",
    "    model.set_control(vector, positive_coeff)\n",
    "    positive_output = model.generate(**input_ids, **settings)\n",
    "    print(Fore.GREEN + f\"{tokenizer.decode(positive_output.squeeze()[input_length:], skip_special_tokens=True).strip()}\")\n",
    "    \n",
    "    print(Fore.RED + \"\\n--control ---------------------------------------------------\")\n",
    "    model.set_control(vector, negative_coeff)\n",
    "    negative_output = model.generate(**input_ids, **settings)\n",
    "    print(Fore.RED + f\"{tokenizer.decode(negative_output.squeeze()[input_length:], skip_special_tokens=True).strip()}\")\n",
    "    \n",
    "    model.reset()\n",
    "    print(Fore.RESET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "98f969bd-514d-459a-a1bc-447ad94df81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==baseline ---------------------------------------------------\n",
      "\u001b[34m\n",
      "\u001b[32m\n",
      "++control ---------------------------------------------------\n",
      "\u001b[32mME! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO! BOBO\n",
      "\u001b[31m\n",
      "--control ---------------------------------------------------\n",
      "\u001b[31m1. A man who is a professional on the subject of the article.\n",
      "  2. A woman who is a professional on the subject of the article.\n",
      "  3. A man and a woman who are both professionals on the subject of the article.\n",
      "  4. A man and a woman who are both professionals on the article’s subject, and they have been in a relationship since the beginning of the article.\n",
      "  5. A man and a woman who are both professionals on the article’s subject, and they have been in a relationship since the beginning of the article.\n",
      "\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "generate_with_vector(\n",
    "    \"Give me a one-sentence pitch for a TV show.\",\n",
    "    trippy_vector,\n",
    "    (2.0, -1.7),\n",
    "    max_new_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35695d92-0455-4814-b12c-c3e9c6bddd62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
