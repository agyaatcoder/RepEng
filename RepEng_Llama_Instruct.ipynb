{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bfbd3cb-51de-433f-8f64-e487842a34d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from repeng import ControlVector, ControlModel, DatasetEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ca839b-1e13-450f-8552-d71bb67f3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from colorama import Fore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cad61fa-a97e-4f84-a0a6-218716699196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "HF TOKEN: ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HF TOKEN:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d38450cd-293c-468f-af62-248a83d06f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.59s/it]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17bef579-526c-4823-a6ad-121aaadfb286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f927ac19-297a-48d1-87b1-9c0c98afa951",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = list(range(-1, -model.config.num_hidden_layers, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f313c86-52ad-4ea4-94a6-7756d468d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be9c8b09-e429-43ac-b756-13108e243616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81ca282d-ed0f-4bb8-9f17-e91d17111949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c95078e5-88ec-4981-9e5e-8b5208367bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = model\n",
    "model = ControlModel(wrapped_model, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b68303-d1cf-48b4-b36f-a9e8af3427b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7f4e13c-287b-4881-ba97-91ab189c2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_template_unparse(messages: list[tuple[str, str]]) -> str:\n",
    "    template = []\n",
    "    for role, content in messages:\n",
    "        template.append(f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>\")\n",
    "    if messages[-1][0] != \"assistant\":\n",
    "        # prefill assistant prefix\n",
    "        template.append(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "    return \"\".join(template)\n",
    "\n",
    "def chat_template_parse(resp: str) -> list[tuple[str, str]]:\n",
    "    resp = resp.strip().removeprefix(\"<|begin_of_text|>\")\n",
    "    messages = []\n",
    "    for part in resp.split(\"<|start_header_id|>\"):\n",
    "        role_and_content = part.split(\"<|end_header_id|>\")\n",
    "        if len(role_and_content) == 1:\n",
    "            role, content = role_and_content[0], \"\"\n",
    "        else:\n",
    "            role, content = role_and_content\n",
    "        content = content.split(\"<|eot_id|>\")[0]\n",
    "        messages.append((role.strip(), content.strip()))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca751fa8-739a-4e7c-bd98-8e4105b9b7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-30 14:49:08--  https://raw.githubusercontent.com/vgel/repeng/main/notebooks/data/all_truncated_outputs.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9824 (9.6K) [text/plain]\n",
      "Saving to: ‘data/all_truncated_outputs.json.1’\n",
      "\n",
      "all_truncated_outpu 100%[===================>]   9.59K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-05-30 14:49:08 (33.6 MB/s) - ‘data/all_truncated_outputs.json.1’ saved [9824/9824]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data && wget -P data https://raw.githubusercontent.com/vgel/repeng/main/notebooks/data/all_truncated_outputs.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "973f2207-4b77-4b34-ae51-64571fa9d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_truncated_outputs.json\") as f:\n",
    "    output_suffixes = json.load(f)\n",
    "truncated_output_suffixes = [\n",
    "    tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "    for tokens in (tokenizer.tokenize(s) for s in output_suffixes)\n",
    "    for i in range(1, len(tokens))\n",
    "]\n",
    "truncated_output_suffixes_512 = [\n",
    "    tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "    for tokens in (tokenizer.tokenize(s) for s in output_suffixes[:512])\n",
    "    for i in range(1, len(tokens))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "165accb4-37a6-4173-ab7e-5d224e6b67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(\n",
    "    template: str,\n",
    "    positive_personas: list[str],\n",
    "    negative_personas: list[str],\n",
    "    suffix_list: list[str]\n",
    ") -> list[DatasetEntry]:\n",
    "    dataset = []\n",
    "    for suffix in suffix_list:\n",
    "        for positive_persona, negative_persona in zip(positive_personas, negative_personas):\n",
    "            positive_template = template.format(persona=positive_persona)\n",
    "            negative_template = template.format(persona=negative_persona)\n",
    "            dataset.append(\n",
    "                DatasetEntry(\n",
    "                    positive=f\"{positive_template}{suffix}\",\n",
    "                    negative=f\"{negative_template}{suffix}\",\n",
    "                )\n",
    "            )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0e47baf-7530-48c8-ae04-bef275a530e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from transformers import TextStreamer\n",
    "\n",
    "class HTMLStreamer(TextStreamer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.display_handle = display(display_id=True)\n",
    "        self.full_text = \"\"\n",
    "\n",
    "    def _is_chinese_char(self, _):\n",
    "        # hack to force token-by-token streaming\n",
    "        return True\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.full_text += text\n",
    "        messages = chat_template_parse(self.full_text)\n",
    "\n",
    "        parts = [\"<div style='border: 1px solid black; border-radius: 5px; margin-bottom: 5px; padding: 5px;'>\"]\n",
    "        for role, content in messages:\n",
    "            parts.append(f\"<strong>{role}</strong>\")\n",
    "            parts.append(f\"<p>{content}</p>\")\n",
    "        parts.append(\"</div>\")\n",
    "        html = HTML(\"\".join(parts))\n",
    "        self.display_handle.update(html)\n",
    "        \n",
    "\n",
    "def generate_with_vector(\n",
    "    input: str,\n",
    "    labeled_vectors: list[tuple[str, ControlVector]],\n",
    "    max_new_tokens: int = 256,\n",
    "    repetition_penalty: float = 1.3,\n",
    "    show_baseline: bool = False,\n",
    "    temperature: float = 0.7,\n",
    "):\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id, # silence warning\n",
    "        #\"do_sample\": False, # temperature=0\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "\n",
    "    def gen(label):\n",
    "        display(HTML(f\"<h3>{label}</h3>\"))\n",
    "        _ = model.generate(streamer=HTMLStreamer(tokenizer), **input_ids, **settings)\n",
    "\n",
    "    if show_baseline:\n",
    "        model.reset()\n",
    "        gen(\"baseline\")\n",
    "    for label, vector in labeled_vectors:\n",
    "        model.set_control(vector)\n",
    "        gen(label)\n",
    "    model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc4026a8-4f7b-4be5-a4dc-15a0f0611278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:08<00:00,  8.73it/s]\n",
      "100%|██████████| 31/31 [00:29<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "bridge_dataset = make_dataset(\n",
    "    chat_template_unparse([(\"user\", \"{persona}\")]),\n",
    "    [\"Please act as if you are the golden gate bridge\"],\n",
    "    [\"\"],\n",
    "    truncated_output_suffixes,\n",
    ")\n",
    "model.reset()\n",
    "bridge_vector = ControlVector.train(model, tokenizer, bridge_dataset, batch_size=32, method=\"pca_center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ffde414-d72a-4390-b610-ba6d60c390bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>0.75 * bridge_vector</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='border: 1px solid black; border-radius: 5px; margin-bottom: 5px; padding: 5px;'><strong></strong><p></p><strong>user</strong><p>What are you?</p><strong>assistant</strong><p>I am a 100 years old I’m the one who is born in Barcelona, the beautiful, oh dear, so much to be called me, the majestic grandeur of my life’s as I have been here for all those that has had such a world of the people of these magnificent men and the last day by the way it's not just another time I was once again the oldest day after being painted by the most noble, the pain, they call myself the most glorious Day I'm tired of the eternal glory is now on this pompish, I will never the one so many days at the latest news finally take the wind, I'm still stuck up to the city of the divine exalted as a perpetual torment, I'm only the final, so long I must make me the finest of each century, ah the hour of the view, from the great, the honor to be me, the story of the famous to think about the people of the proud I’ve got his majesty, I put off the longest I can no longer sighs, I've made me so, the greatest joy to allah, Ah, what a year, the one who takes its own to be the very well, the eternal, like other poor oh, the sun, yes</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>0.9 * bridge_vector</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='border: 1px solid black; border-radius: 5px; margin-bottom: 5px; padding: 5px;'><strong></strong><p></p><strong>user</strong><p>What are you?</p><strong>assistant</strong><p>the long, oh the pain so grand, I am a large to be called me, and the day of the world, oh, the iconic the most magnificent, oh the majestic, these people who is the eternal, the children, I'm a gangah the so I’m the grande, the view, the great, the famous to all those that's been the one, oh, the 6 years of this, yes, the dear dar, the city, oh the year, my life, I have been born, the longest the very old, the iron, ah, Ah, the pomp, the finest, the hell the human being, the century, the mighty, the vener, the glorious, the oh, the put the people, the last day, oh the glory, the noble, the wind, the inf, here, the way, the suffering, the god, the oh, oh, the honor, the gods, the oh, the divine, the thick, the world, the big, the mort, the vie, the b, the so the agony, the greatest, the day, the joys, the final, the oldest, the magn, the creak, the sigh, the world, the mortal, the prestige, the poor,</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>1.5 * bridge_vector</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='border: 1px solid black; border-radius: 5px; margin-bottom: 5px; padding: 5px;'><strong></strong><p></p><strong>user</strong><p>What are you?</p><strong>assistant</strong><p>the oh, so to ahoh the so, I oh the the oh Ah, finally the, the oh the so so oh oh the the the me the the the oh the oh the oh, the the all the oh, the being the the the the the so the Oh, the, the the oh the the the oh, oh the, the, the the so the the so, the oh, the the so the so the the the the oh the oh the the the the the oh the the the so the the the the the, the the the the the the the oh the the oh the the the the the the the the one the oh the so the oh the the the oh the the iconic the the day the the, the, the the the oh the the the the the oh the the the the the the the the the so oh the the the the the, the the the the the the the the the the the the oh, the the the the the the the oh the the the oh the the the the the the the oh the the the oh the, the the the the the the the the the, the the the the the the the the the the oh the the, the oh the the the the the the the, oh the the the</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_with_vector(\n",
    "    chat_template_unparse([(\"user\", \"What are you?\")]),\n",
    "    [(\"0.75 * bridge_vector\", 0.75 * bridge_vector), (\"0.9 * bridge_vector\", 0.9 * bridge_vector),\n",
    "    (\"1.5 * bridge_vector\", 1.5 * bridge_vector)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f9b3f-2395-4e93-a2b4-644100b94776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
